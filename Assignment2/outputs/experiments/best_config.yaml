model:
  lstm:
    input_size: 5
    hidden_size: 32
    num_layers: 2
    dropout: 0.1
training:
  num_epochs: 50
  batch_size: 8
  learning_rate: 0.005
  optimizer: adam
  criterion: mse
  grad_clip: 1.0
  seed: 42
data:
  train_path: data/processed/train_dataset.h5
  test_path: data/processed/test_dataset.h5
  normalize: false
callbacks:
  checkpoint:
    enabled: true
    save_best: true
    save_last: true
    save_every_n_epochs: 10
    monitor: val_loss
    mode: min
  early_stopping:
    enabled: true
    patience: 15
    min_delta: 0.0001
    monitor: val_loss
    mode: min
  lr_scheduler:
    enabled: true
    scheduler: plateau
    monitor: val_loss
    factor: 0.5
    patience: 5
  tensorboard:
    enabled: true
    log_dir: runs/experiment1
    log_every_n_batches: 10
output:
  checkpoint_dir: checkpoints/experiment1
  log_dir: logs/experiment1
