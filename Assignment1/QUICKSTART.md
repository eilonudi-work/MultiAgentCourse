# ğŸš€ Quick Start Guide

## What Was Fixed

The `start-dev.sh` script was stopping at "Installing backend dependencies..." because:

1. âŒ **Missing `backend/requirements.txt`** - The Python dependencies file was missing
2. âŒ **Silent failures** - Errors were hidden by redirecting output to `/dev/null`
3. âŒ **No error messages** - The script didn't show why installations failed

### âœ… Fixes Applied

1. âœ… Created `backend/requirements.txt` with all required Python dependencies
2. âœ… Updated `start-dev.sh` to show installation progress
3. âœ… Added proper error checking and helpful error messages
4. âœ… Shows last 20 lines of logs if services fail to start

---

## How to Start the Application

### Option 1: Automated Script (Recommended)

```bash
cd "/Users/eilonudi/Desktop/HW/LLMs in multiagent env/MultiAgentCourse/Assignment1"
./start-dev.sh
```

**What the script does:**
1. âœ… Checks prerequisites (Python, Node.js, Ollama)
2. âœ… Creates Python virtual environment
3. âœ… Installs backend dependencies (~2-3 minutes first time)
4. âœ… Installs frontend dependencies (~2-3 minutes first time)
5. âœ… Creates environment files
6. âœ… Starts backend on http://localhost:8000
7. âœ… Starts frontend on http://localhost:5173

### Option 2: Manual Start

**Backend:**
```bash
cd backend

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start backend
python run.py
```

**Frontend (in a new terminal):**
```bash
cd frontend

# Install dependencies
npm install

# Start frontend
npm run dev
```

---

## Expected Output

When you run `./start-dev.sh`, you should see:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ¦™ Ollama Web GUI - Development Startup              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Checking prerequisites...
âœ“ Python 3 found: Python 3.x.x
âœ“ Node.js found: v18.x.x
âœ“ npm found: 9.x.x
âœ“ Ollama found: ollama version x.x.x

â„¹ Checking ports...
âœ“ Port 8000 is available (backend)
âœ“ Port 5173 is available (frontend)

â„¹ Checking Ollama service...
âœ“ Ollama is running on port 11434

â„¹ Setting up backend...
â„¹ Creating Python virtual environment...
âœ“ Virtual environment created
â„¹ Activating virtual environment...
â„¹ Installing backend dependencies (this may take a few minutes)...
  Upgrading pip...
  Installing requirements...
âœ“ Backend dependencies installed
âœ“ .env file exists
â„¹ Initializing database...
âœ“ Database initialized

â„¹ Setting up frontend...
â„¹ Installing frontend dependencies (this may take a few minutes)...
âœ“ Frontend dependencies installed
âœ“ .env file exists

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ¦™ Ollama Web GUI - Development Startup              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Starting services...

â„¹ Starting backend on http://localhost:8000...
âœ“ Backend started successfully
â„¹   API: http://localhost:8000
â„¹   Docs: http://localhost:8000/docs
â„¹   Health: http://localhost:8000/health

â„¹ Starting frontend on http://localhost:5173...
âœ“ Frontend started successfully
â„¹   URL: http://localhost:5173

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Service Status                                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Backend:   http://localhost:8000                       â•‘
â•‘  Frontend:  http://localhost:5173                       â•‘
â•‘  API Docs:  http://localhost:8000/docs                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â„¹ Logs are available in:
â„¹   Backend:  logs/backend.log
â„¹   Frontend: logs/frontend.log

â„¹ Press Ctrl+C to stop all services
```

---

## First Time Setup

Once the application is running:

1. **Open your browser** to http://localhost:5173

2. **You'll see the setup screen**
   - Enter an API key (any string, e.g., `my-secret-key-123`)
   - Enter Ollama URL: `http://localhost:11434`

3. **Click "Test Connection"**
   - Should show "Connection successful" if Ollama is running

4. **Click "Save Configuration"**
   - You'll be redirected to the chat interface

5. **Start chatting!**
   - Select a model from the dropdown
   - Type a message and press Enter
   - Watch the response stream in real-time

---

## Troubleshooting

### Issue 1: "Python 3 is not installed"

**Solution:** Install Python 3.10 or higher
```bash
# macOS (using Homebrew)
brew install python@3.10

# Or download from python.org
```

### Issue 2: "Node.js is not installed"

**Solution:** Install Node.js 18 or higher
```bash
# macOS (using Homebrew)
brew install node

# Or download from nodejs.org
```

### Issue 3: "Ollama doesn't seem to be running"

**Solution:** Start Ollama
```bash
# If installed via CLI
ollama serve

# If installed via app, it should be running in background
# Check: curl http://localhost:11434/api/tags
```

### Issue 4: "Port 8000 is already in use"

**Solution:** Kill the process using port 8000
```bash
lsof -ti:8000 | xargs kill -9
```

### Issue 5: "Port 5173 is already in use"

**Solution:** Kill the process using port 5173
```bash
lsof -ti:5173 | xargs kill -9
```

### Issue 6: "Failed to install backend dependencies"

**Solution:** Install manually and check for errors
```bash
cd backend
source venv/bin/activate
pip install -r requirements.txt
# Look at any error messages
```

### Issue 7: "Failed to install frontend dependencies"

**Solution:** Install manually and check for errors
```bash
cd frontend
npm install
# Look at any error messages
```

### Issue 8: "Backend failed to start"

**Check the logs:**
```bash
cat logs/backend.log
```

**Common causes:**
- Missing dependencies
- Port already in use
- Database initialization failed
- Environment file issues

### Issue 9: "Frontend failed to start"

**Check the logs:**
```bash
cat logs/frontend.log
```

**Common causes:**
- Missing dependencies
- Port already in use
- Vite configuration issues

### Issue 10: Script hangs during installation

**Solution:**
1. Press Ctrl+C to stop the script
2. Delete the `venv/installed` marker file:
   ```bash
   rm backend/venv/installed
   ```
3. Run the script again:
   ```bash
   ./start-dev.sh
   ```

---

## Stopping the Application

Press **Ctrl+C** in the terminal where `start-dev.sh` is running.

The script will automatically:
- Stop the backend server
- Stop the frontend server
- Clean up background processes

---

## Resetting Everything

If you want to start fresh:

```bash
# Remove virtual environment
rm -rf backend/venv

# Remove node modules
rm -rf frontend/node_modules

# Remove database
rm backend/ollama_web.db

# Remove logs
rm -rf logs

# Run the script again
./start-dev.sh
```

---

## Next Steps

1. âœ… Run `./start-dev.sh`
2. âœ… Wait for "All services started successfully!"
3. âœ… Open http://localhost:5173
4. âœ… Complete setup (API key + Ollama URL)
5. âœ… Start chatting with your local models!

---

## Testing the Integration

After starting the application, you can run integration tests:

```bash
# In a new terminal
cd "/Users/eilonudi/Desktop/HW/LLMs in multiagent env/MultiAgentCourse/Assignment1"
./test-integration.sh
```

Expected result: **18/18 tests passing** âœ…

---

## Documentation

For more information, see:
- **`README.md`** - Complete project documentation
- **`INTEGRATION_GUIDE.md`** - Detailed integration guide
- **`INTEGRATION_STATUS.md`** - Integration verification
- **`backend/API_ENDPOINTS.md`** - API reference
- **`backend/DEPLOYMENT.md`** - Production deployment

---

## Support

If you encounter issues:
1. Check this quickstart guide
2. Look at the logs in `logs/` directory
3. Review the detailed documentation
4. Check `INTEGRATION_GUIDE.md` troubleshooting section

---

**Ready? Let's start!** ğŸš€

```bash
./start-dev.sh
```
