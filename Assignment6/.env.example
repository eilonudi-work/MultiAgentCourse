# Ollama Configuration
OLLAMA_HOST=http://localhost:11434

# Model Configuration
# Popular models: llama2, mistral, phi, gemma, llama3, etc.
MODEL_NAME=llama2

# Rate Limiting (for local models, can be lower)
REQUEST_DELAY=0.1
